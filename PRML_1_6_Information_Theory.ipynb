{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRML_1_6_Information_Theory.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9Gm0tfU8cZCz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1-6. 정보 이론"
      ]
    },
    {
      "metadata": {
        "id": "h3c6Mu_wRIa5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Information $h(x)$\n",
        "* 주어진 랜덤 변수 $x$ 가 있고, 이 변수로부터 관찰하여 얻는 정보의 값\n",
        "\n",
        "* 정보의 양 = 놀라움의 정도\n",
        "    * 높은 확률로 일어날 사건이 발생함 vs 매우 낮은 확률로 일어날 사건이 발생함 (ex. 지구와 소행성이 충돌하는 사건)\n",
        "    \n",
        "* 이렇게 정보량의 측정 단위는 확률 분포 $p(x)$로 표현 가능 (두 사건이 독립적이라면)\n",
        "\n",
        "$$\n",
        "p(x,y) = p(x)p(y) \\\\\n",
        "h(x,y) = h(x) + h(y)\n",
        "$$\n",
        "\n",
        "\n",
        "* 위의 관계로 부터 $h(x)$는 $p(x)$의 로그에 해당함\n",
        "    * $(-)$ 부호는 정보량이 음의 값을 가지기 않기 위해\n",
        "    * 사건 $x$의 확률이 낮을수록 얻는 정보량은 큼\n",
        "\n",
        "$$\n",
        "h(x) = -log_{2}p(x)\n",
        "$$\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "metadata": {
        "id": "oWT75cGmfEmo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Entropy $H(x)$\n",
        "* 정보를 전달해야하는 상황일때, 전송에 필요한 정보량의 평균\n",
        "\n",
        "$$\n",
        "H[x] = - \\sum_{x} p(x) \\mathrm{log}_{2} p(x)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H[x] = - \\int p(\\mathbf{x}) \\mathrm{ln} p(\\mathbf{x}) d\\mathbf{x}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yTS2us6mfsty",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. 노이즈 없는 코딩 이론\n",
        "* 엔트로피는 확률 변수의 상태를 전송하기 위해 필요한 비트 숫자의 하한선\n",
        "\n",
        "* 서로 다른 8 종류의 정보를 가진 동일한 확률 정보 $x$에 대해\n",
        "\n",
        "$$\n",
        "H[x] = -8 \\times \\frac{1}{8} \\mathrm{log}_{2} \\frac{1}{8} = 3 \\mathrm{bits}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "pWQ12Z1yjjAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. 물리학에서의 엔트로피\n",
        "* $N$개의 동일한 물체가 몇 개의 통에 담겨 있고, $i$번째 통 안에 $n_{i}$개의 물체가 담겨있다. 물체를 통 안에 담는 방법의 가짓수에 대해 생각해보면,\n",
        "\n",
        "$$\n",
        "\\eqalign {} {\n",
        "W = \\frac{N!}{\\prod_{i} n_{i}!} \\\\\n",
        "\\mathrm{ln}N! \\simeq N \\mathrm{ln}N - N \\\\\n",
        "\\sum_{i} n_{i} =N \\\\\n",
        "H = \\frac{1}{N} \\mathrm{ln} W = \\frac{1}{N} \\mathrm{N}! - \\frac{1}{N}\\sum_{i} \\mathrm{ln}n_{i}! \\\\\n",
        "\\quad = \\lim_{N \\rightarrow \\infty} \\begin{Bmatrix} \\frac{1}{N} \\mathrm{ln} N! - \\frac{1}{N}\\sum_{i}\\mathrm{ln}n_{i}!\\end{Bmatrix} \\\\\n",
        "\\quad = \\lim_{N \\rightarrow \\infty} \\begin{Bmatrix} \\frac{1}{N}(N\\mathrm{ln}N-N) - \\frac{1}{N}\\sum_{i}(n_{i}\\mathrm{ln}n_{i}-n_{i})\\end{Bmatrix} \\\\\n",
        "\\quad = \\lim_{N \\rightarrow \\infty} \\begin{Bmatrix} \\mathrm{ln}N - 1 - \\frac{1}{N}(\\sum_{i}n_{i}\\mathrm{ln}n_{i}-\\sum_{i}n_{i})\\end{Bmatrix} \\\\\n",
        "\\quad = \\lim_{N \\rightarrow \\infty} \\begin{Bmatrix} \\frac{\\Sigma_{i}n_{i}}{N}\\mathrm{ln}N - 1 - \\frac{1}{N}\\sum_{i}n_{i}\\mathrm{ln}n_{i}+1\\end{Bmatrix} \\\\\n",
        "\\quad = \\lim_{N \\rightarrow \\infty} \\begin{Bmatrix} -\\sum_{i} \\frac{n_{i}}{N}\\mathrm{ln}(\\frac{n_{i}}{N})\\end{Bmatrix} \\\\\n",
        "\\quad = - \\sum_{i} p_{i} \\mathrm{ln} p_{i}\n",
        "}\n",
        "$$\n",
        "\n",
        "* 각각의 통을 확률 변수 $X$의 상태 $x_{i}$라고 해석하고, $p(X=x_{i})=p_{i}$일 경우,\n",
        "\n",
        "$$\n",
        "H[p] = - \\sum_{i} p(x_{i}) \\mathrm{ln} p(x_{i})\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "3TNQq2YbtZfi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* 그림 1.30에서 볼 수 있는 것처럼 분포 $p(x_{i})$가 몇몇 값에 뾰족하게 집중되어 있는 경우에는 상대적으로 낮은 Entropy, 더 많은 값들 사이에 퍼져 있을 때는 높은 Entorpy를 가지게 된다.\n",
        "\n",
        "* $0 \\le p_{i} \\le 1$이기 때문에 $\\mathrm{Entropy} \\ge 0$.\n",
        "\n",
        "* Entropy가 최대가 되는 경우는 라그랑주 승수법을 활용하여 $H$의 최댓값을 찾아낼 수 있다. $x_{i}$의 상태의 가짓수가 $M$이라면,\n",
        "\n",
        "$$\n",
        "\\forall i : p_{i} = \\frac{1}{M}\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "XnzSKq0ywNuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. 미분 엔트로피\n",
        "\n",
        "* $x$가 너비 $\\Delta$의 여러 구간으로 나누어져 있다면,\n",
        "\n",
        "$$\n",
        "\\lim_{\\Delta \\rightarrow 0} \\begin{Bmatrix} - \\sum_{i} p(x_{i}) \\Delta \\mathrm{ln} p(x_{i})\\end{Bmatrix} = - \\int p(\\mathbf{x}) \\mathrm{ln} p(\\mathbf{x}) d\\mathbf{x}\n",
        "$$\n",
        "\n",
        "* 미분 엔트로피를 최대화 하려면,\n",
        "$$\n",
        "p(x) = \\mathcal{N}(x | \\mu, \\sigma^{2})\n",
        "$$\n",
        "\n",
        "* 가우시안 분포를 따르게 되고, 그 엔트로피를 구하면,\n",
        "$$\n",
        "H[x] = \\frac{1}{2}\\begin{Bmatrix} 1+\\mathrm{ln}(2\\pi \\sigma^{2})\\end{Bmatrix}\n",
        "$$\n",
        "\n",
        "* 분포가 더 넓게 퍼져 있을수록 ($\\sigma^{2}$가 클수록) 엔트로피가 증가\n",
        "* 미분 엔트로피는 이산 엔트로피와는 달리 음의 값도 가질 수 있다.\n",
        "\n",
        "$$\n",
        "\\sigma^{2} < \\frac{1}{2\\pi e}, \\qquad H(x) < 0\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "2XfufJFDzUF5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. 조건부 엔트로피\n",
        "\n",
        "* $\\mathbf{x}$ 값과 $\\mathbf{y}$ 값을 함께 뽑는 결합 분포 $p(\\mathbf{x}, \\mathbf{y})$, 만약 $\\mathbf{x}$의 값이 이미 알려져 있다면,\n",
        "\n",
        "$$\n",
        "H[\\mathbf{y}|\\mathbf{x}] = - \\int \\int p(\\mathbf{y},\\mathbf{x}) \\mathrm{ln} p(\\mathbf{y}|\\mathbf{x}) d\\mathbf{y} d\\mathbf{x} \\\\\n",
        "H[\\mathbf{x},\\mathbf{y}] = H[\\mathbf{y}|\\mathbf{x}] + H[\\mathbf{x}]\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "RRyYNFlg04Ia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. 상대적 엔트로피 또는 KL 발산\n",
        "\n",
        "\n",
        "* 알려지지 않은 분포 $p(\\mathbf{x})$를 피팅하기 위해 모델을 만들었고, 그 결과 분포 $q(\\mathbf{x})$를 구할 수 있었다면, 추가로 필요한 정보의 양은,\n",
        "\n",
        "$$\n",
        "\\eqalign {}{\n",
        "KL(p||q) = \\quad - \\int p(\\mathbf{x}) \\mathrm{ln} q(\\mathbf{x}) d\\mathbf{x} - \\begin{pmatrix} - \\int p(\\mathbf{x}) \\mathrm{ln} p(\\mathbf{x}) d\\mathbf{x}  \\end{pmatrix} \\\\\n",
        "\\qquad \\quad = - \\int p(\\mathbf{x}) \\mathrm{ln} \\begin{Bmatrix} \\frac{q(\\mathbf{x})}{p(\\mathbf{x})}\\end{Bmatrix} d\\mathbf{x}\n",
        "}\n",
        "$$\n",
        "\n",
        "$$\n",
        "KL(p||q) \\simeq \\frac{1}{N} \\sum_{n=1}^{N} \\begin{Bmatrix} -\\mathrm{ln}q(\\mathbf{x}_{n}|\\mathbf{\\theta}) + \\mathrm(ln) p(\\mathbf{x}_{n})\\end{Bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "KL(p||q) \\ge 0, \\qquad KL(p||q) \\not\\equiv KL(q||p)\n",
        "$$\n",
        "\n",
        "* 두개의 분포 $p(\\mathbf{x})$, $q(\\mathbf{x})$의 비유사도를 측정하기 위해 사용\n",
        "\n",
        "*  $KL(p||q) \\ge 0$ 에서 $KL(p||q) = 0$일 때, $p(\\mathbf{x}) = q(\\mathbf{x})$"
      ]
    },
    {
      "metadata": {
        "id": "GDfJYf5d5Qz-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8. 상호 정보량\n",
        "\n",
        "* 두 변수 $\\mathbf{x}$와 $\\mathbf{y}$의 결합 분포 $p(\\mathbf{x}, \\mathbf{y})$가 독립이라면, $p(\\mathbf{x},\\mathbf{y}) = p(\\mathbf{x})p(\\mathbf{y})$가 성립, 그렇지 않다면,\n",
        "\n",
        "$$\n",
        "\\eqalign {}{\n",
        "I[\\mathbf{x}, \\mathbf{y}] \\equiv KL(p(\\mathbf{x}, \\mathbf{y}) || p(\\mathbf{x})p(\\mathbf{y}))  \\\\\n",
        "\\qquad =  - \\int\\int p(\\mathbf{x}, \\mathbf{y}) \\mathrm{ln} \\begin{pmatrix} \\frac{p(\\mathbf{x})p(\\mathbf{y})}{p(\\mathbf{x}, \\mathbf{y})} \\end{pmatrix} d\\mathbf{x} d\\mathbf{y}\n",
        "}\n",
        "$$\n",
        "\n",
        "* KL 발산의 성질에 따라서 $I[\\mathbf{x}, \\mathbf{y}] \\ge 0$이며, $\\mathbf{x}$, $\\mathbf{y}$가 서로 독립적일 때만 $I[\\mathbf{x}, \\mathbf{y}] =0 $\n",
        "* $\\mathbf{y}$에 대해 알고 있을 때 $\\mathbf{x}$ 값에 대한 불확실성을 표현한 것이 상호 정보량\n",
        "\n",
        "$$\n",
        "I[\\mathbf{x}, \\mathbf{y}] = H[\\mathbf{x}] - H[\\mathbf{x}|\\mathbf{y}] = H[\\mathbf{y}] - H[\\mathbf{y}|\\mathbf{x}]\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "EbRfKLqU78Ge",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 9. 참고\n",
        "* https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-slides-1.pdf\n",
        "* https://www.cnblogs.com/glory-of-family/p/5602316.html"
      ]
    }
  ]
}
